# How many times your classifier is confused - confuse matrix

# perfect confusion matrix:-
#                predict-neg     predict-pos
#    actual-neg  |   m               0      |
#    actual-pos  |   0               n      |

# done

# precision - percentage of positive-predictions were correct
# in mathematical term, precision = true-precision / (true-precision + false-prediction)

# recall - percentage of actual-values classified by the classifier
# in mathematical term, recall = true-positive / (true-positive + false-positive)

# F1-score - hermonic mean of precision and recall
# in mathematical term, F1-score = (2 * precision * recall) / (precision + recall)
